\documentclass{beamer}

% If you use the optional handout parameter below, none of the pauses between slides are printed.  Only one of these first two lines should be used at a time.
% \documentclass[handout]{beamer}

\setbeamertemplate{navigation symbols}{}

\usepackage{amsmath, amsthm, amssymb, upgreek, amscd, verbatim, xcolor}
\usepackage{centernot}
\usepackage{datetime}
\usepackage{fontspec}
\usepackage{graphicx} 
\usepackage{float} 
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{unicode-math} 
\usepackage{extarrows}
\usepackage{graphicx}

\usetheme{Madrid}
\usecolortheme{seahorse}
\setbeamertemplate{items}[circle]
\usefonttheme{serif}

% Use the XITS font
\setmathfont{XITSMath-Regular}
\setmainfont{XITS}

% File metadata
\newcommand{\DocumentTitle}{PageRank Algorithm}
\newcommand{\DocumentAuthor}{Guxiao Hu, Puyuan Zhang, Xiu Chen, Zipei Zhu}

\hypersetup{
pdftitle={\DocumentTitle},
% pdfsubject={lorem ipsum},  % TODO
pdfauthor={\DocumentAuthor}, 
% pdfkeywords={lorem ipsum}  % TODO
}

%New Commands
\newcommand{\ssm}{\,{}^\smallsetminus\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}

\begin{document}
\title{\textbf\DocumentTitle}
\author{\DocumentAuthor}
\date{\today}

\maketitle

\begin{frame}{Table of Contents}
    \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{How do search engines work?}
    \textit{You are an engineer in {\color{blue}{G}}{\color{orange}{u}}{\color{red}{g}}{\color{green}{l}}{\color{blue}{o}}{\color{red}{o}} designing a search engine in the 1990s.}

    Consider: how to rank results for each query?
    \pause
    \begin{itemize}
        \item Relevance
        \pause
            \begin{itemize}
                \item NLP-focused approaches. May be discussed at the end of this pre (also linear-algebra heavy!).
            \end{itemize}
        \pause
        \item Importance
        \pause
            \begin{itemize}
                \item Sorting on accessing counts?
                \item Sorting on query term coverage (\# query terms appear in each result)?
            \end{itemize}
    \end{itemize}  
    \pause
    PageRank algorithm focuses on \textit{how important} each webpage is.
\end{frame}

\section{Algorithm Overview}

\begin{frame}{The ``Random Surfer'' Model}
    The PageRank algorithm 
    \begin{itemize}
        \item iterates on a \textbf{graph} where webpages are \textbf{nodes}, and links are \textbf{directed edges},
        \item calculates the probability of a ``random surfer'' visiting each webpage,
        \item outputs a probability distribution vector.
                $$
                    \boldsymbol p = \begin{bmatrix}
                    \Pr(1) & \Pr(2) & \cdots & \Pr(n)
                    \end{bmatrix}^\mathsf T
                $$
    \end{itemize}
\end{frame}

\begin{frame}{Algorithm}
    \begin{block}{Notations and Conventions}
        \begin{itemize}
            \item Let $n$ denote the number of nodes (i.e. total number of webpages).
            \item Let $\deg^+(u)$ denote the \textbf{out degree} of node $u$ (i.e. number of outreaching links on webpage $u$).
        \end{itemize}
    \end{block}

    \begin{definition}
        \textbf{Google PageRank\footnote{Trademark of Google; U.S. patent 6,285,999.} iteration}: Initially, $\Pr(i) := 1 / n$ for each webpage $1 \le i \le n$. Then iterate by
        $$
            \Pr(v) := (1 - d)\frac{1}{n} + d\sum_{\text{edge $u\to v$}} \frac{1}{\deg^+(u)}\Pr(u)
        $$
        where $d$ is the \textbf{damping factor} ($0 < d < 1$). 
    \end{definition}
\end{frame}

\section{Computing PageRank}

\begin{frame}{Computing PageRank}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item A node is important if important nodes point to it.
                \item A node contributes part of its importance to the nodes it points to.
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/PageRanks-Example.png}
        \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}[allowframebreaks]{Matrix Algebra}
    \begin{block}{A Simplified Model}
        Let \textbf{transition matrix}
        $$
            \mathcal M_{ij} := \begin{cases}
                \dfrac{1}{\deg^+ (j)} & \text{edge $j \to i$}\\
                0 & \text{otherwise}
            \end{cases}
        $$
        i.e., given \textbf{adjacent matrix} $A_{ij} = [\text{edge $i\to j$}]$ and diagonal matrix $K$  with the outdegrees in the diagonal,
        $$
            \mathcal M := (K^{-1}A)^\mathsf T
        $$
    \end{block}
    \begin{definition}
        Let \textbf{probability distribution vector} of the $k$-th iteration be
        $$
            \boldsymbol p(k) := \begin{bmatrix}
                \Pr(1) & \Pr(2) & \cdots & \Pr(n)
            \end{bmatrix}^\mathsf T
        $$
        which is initially set to
        $$
        \boldsymbol p(0) := \begin{bmatrix}
            1/n & 1/n & \cdots & 1/n
        \end{bmatrix}^\mathsf T
        $$
    \end{definition}
    \begin{definition}
        The \textbf{Google matrix} $\widehat{\mathcal M}$ is defined by
        $$
            \widehat{\mathcal M} := d\mathcal M + \frac{1 - d}{n} E
        $$
        where $E$ is $n \times n$ matrix of all ones (so that $E\boldsymbol p = \boldsymbol 1$), and $0 < d < 1$ is the damping factor. 
    \end{definition}

    \begin{block}{The Power Method}
        Google PageRank can be computed by
        $$
            \boldsymbol p(k + 1) = \widehat{\mathcal M}\boldsymbol p(k)
        $$
        One may assume PageRank converges after $|\boldsymbol p(t) - \boldsymbol p(t - 1)| < \epsilon$.
    \end{block}
\end{frame}

\section{Mechanisms}

\begin{frame}{PageRank and Eigenvectors}
    Recall the equation
    $$
    \begin{aligned}
        \boldsymbol p(k + 1) &= \widehat{\mathcal M}\boldsymbol p(k)\\
        \pause
        \Rightarrow \boldsymbol p &= \widehat{\mathcal M}\boldsymbol p \quad \text{(when converged)}
    \end{aligned}
    $$
    \pause
    Iteration is ``finding a vector that remains unchanged by linear transformation $\widehat{\mathcal M}$''.
    \pause
    \begin{definition}
        $\boldsymbol v$ is the \textbf{eigenvector}, and $\lambda$ is the \textbf{eigenvalue} of matrix $A$, if
        $$
            A\boldsymbol v = \lambda \boldsymbol v
        $$
    \end{definition}
    \pause
    In fact, $\boldsymbol p$ is the \textbf{principal eigenvector} of the Google matrix $\widehat{\mathcal M}$, and $1$ is the corresponding eigenvalue (i.e. the eigenvalue with the largest magnitude).
\end{frame}

\begin{frame}{Properties}
    \begin{fact}
        \begin{itemize}
            \item Both $\mathcal M$ and $\widehat{\mathcal M}$ are \textbf{column-stochastic} (sums of each column are $1$), positive.
                $$
                \sum_i \widehat{\mathcal M}_{ij} = 1 \  (\forall j) \Longleftrightarrow \boldsymbol{1}^\mathsf T \widehat{\mathcal M} = \boldsymbol{1}^\mathsf T
                $$
                ($\boldsymbol{1}^\mathsf T$ is a \textbf{left eigenvector} with eigenvalue $1$ $\Rightarrow$ $1$ is an eigenvalue of $\widehat{\mathcal M}$)
            \item $\widehat{\mathcal M}$ is positive matrix. 
        \end{itemize}
    \end{fact}
    By the \textbf{Perronâ€“Frobenius theorem}, $\widehat{\mathcal M}$ satisfies
    \begin{itemize}
        \item all other eigenvalues $\lambda_i$ satisfy $|\lambda_i| < 1$,
        \item the corresponding eigenvector has all positive entries.
    \end{itemize}
\end{frame}

\begin{frame}{A View on Probability}
    Matrix multiplication $\to$ updating probability distribution vector.
    \pause

    Reason to use the damping factor? \pause To avoid trapping. 
    \pause
    \begin{itemize}
        \item With a probability of $d$, ``random surfing'' continues;
        \begin{itemize}
            \item Each $u$ has equal chance $1/\deg^+(u)$ to redirect to $v$
        \end{itemize}
        \item with a probability of $1 - d$, surfer redirects to another random page.
        \begin{itemize}
            \item Each of the $n$ pages has equal chance to redirect to the current page.
        \end{itemize}
    \end{itemize}
    \pause
    $$
        \begin{aligned}
            \mathbb P(v) &= \mathbb P(v | \text{randomly redirected}) + \sum_{\text{edge $u \to v$}} \mathbb P(v | u) \\
             &= (1 - d)\frac{1}{n} + 
            d\sum_{\text{edge $u\to v$}} \frac{1}{\deg^+(u)}\mathbb P(u)
        \end{aligned}
    $$
\end{frame}

\section{Applications and Extensions}

\begin{frame}{Applications}
    Generalizing \textit{importance}. 
    \pause
    \begin{itemize}
        \item Evaluating academic papers based on citations.
        \item Determine species that are essential to the continuing health of ecosystems.
        \item Ranking performance of sports teams.
        \item ... 
    \end{itemize}
\end{frame}

\begin{frame}{Further Reading I}
    Suggested topics for further reading:
    \begin{block}{Topic}
        PageRank is a variation of \textbf{the Markov Chains}.
    \end{block}
    \begin{fact}
        Random walk which is
        \begin{itemize}
            \item \textbf{irreducible}: with probability of $1/d$, surfer can reach any page with one step $\Rightarrow$ all pages are reachable ($\Pr > 0$) to the surfer
            \item \textbf{aperiodic}: with probability of $d$, surfer stops on their current page $\Rightarrow$ no fixed-length cycles in transition
            \item \textbf{positive recurrent}: expected return time to any page is finite
        \end{itemize}
        converges to a stationary distribution.
    \end{fact}
\end{frame}

\begin{frame}{Question}
    Will the simplified model $\boldsymbol p(k + 1) = \mathcal M \boldsymbol p(k)$  we defined before always converge?
    \pause

    \textit{If not,} what will happen and why? Construct test-cases to prove.
\end{frame}

\begin{frame}{Further Reading II}
    \begin{block}{Topic}
        Some Easy Ways to Briefly Analyze ``Relevance'':
        \begin{itemize}
            \item Word frequency (TF / TF-IDF)
            \item Proximity scoring (word vectors)
            \item Matching phrases (bags-of-words)
            \item ...
        \end{itemize}
    \end{block}
\end{frame}

\section{Ending}

\begin{frame}{Ending}
    \begin{center}
        {\Huge{Thanks!}}
    \end{center}
\end{frame}

\end{document}